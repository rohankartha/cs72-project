{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPT-4o-Mini w/ Augmentation** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "from config import DARTMOUTH_API_KEY, DARTMOUTH_CHAT_API_KEY\n",
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "# Retrieving keys and creating environment variables\n",
    "os.environ['DARTMOUTH_CHAT_API_KEY'] = DARTMOUTH_CHAT_API_KEY\n",
    "os.environ['DARTMOUTH_API_KEY'] = DARTMOUTH_API_KEY\n",
    "\n",
    "# Defining llm and embeddings models\n",
    "llm_model_name = \"openai.gpt-4o-mini-2024-07-18\"\n",
    "embeddings_model_name = \"bge-m3\"\n",
    "\n",
    "# Defining keywords and sources\n",
    "keywords = \"social security OR government welfare OR medicare OR medicaid\"\n",
    "source=\"breitbart-news\"\n",
    "\n",
    "# Defining testing data file\n",
    "testing_data = '../input/govtspending.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'llama-3-8b-instruct', 'provider': 'meta', 'display_name': 'Llama 3 8B Instruct', 'tokenizer': 'meta-llama/Meta-Llama-3-8B-Instruct', 'type': 'llm', 'capabilities': ['chat'], 'server': 'text-generation-inference', 'parameters': {'max_input_tokens': 8192}}, {'name': 'llama-3-2-11b-vision-instruct', 'provider': 'meta', 'display_name': 'Llama 3.2 11B Vision Instruct', 'tokenizer': 'meta-llama/Llama-3.2-11B-Vision-Instruct', 'type': 'llm', 'capabilities': ['chat', 'vision'], 'server': 'text-generation-inference', 'parameters': {'max_input_tokens': 127999}}, {'name': 'codellama-13b-instruct-hf', 'provider': 'meta', 'display_name': 'CodeLlama 13B Instruct HF', 'tokenizer': 'meta-llama/CodeLlama-13b-Instruct-hf', 'type': 'llm', 'capabilities': ['chat'], 'server': 'text-generation-inference', 'parameters': {'max_input_tokens': 6144}}, {'name': 'codellama-13b-python-hf', 'provider': 'meta', 'display_name': 'CodeLlama 13B Python HF', 'tokenizer': 'meta-llama/CodeLlama-13b-Python-hf', 'type': 'llm', 'capabilities': [], 'server': 'text-generation-inference', 'parameters': {'max_input_tokens': 2048}}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_dartmouth.llms import DartmouthLLM\n",
    "print(DartmouthLLM.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from newsapi import NewsApiClient\n",
    "import re\n",
    "\n",
    "# Creating directory to hold scraped articles\n",
    "os.makedirs(name=f\"../knowledge-bases/{keywords}_{source}\")\n",
    "\n",
    "# Creating newsapi client\n",
    "newsapiclient = NewsApiClient(api_key=\"03122dc3b7b84ea29212ca965b40c7aa\")\n",
    "\n",
    "# Querying articles\n",
    "articles = newsapiclient.get_everything(q=keywords, sources=source ,page_size=100)\n",
    "articles = articles['articles']\n",
    "\n",
    "# Scraping articles and saving in directory\n",
    "for article in articles:\n",
    "\turl = article['url']\n",
    "\tresponse = requests.get(url=url)\n",
    "\n",
    "\t# Printing article content to directory if valid response\n",
    "\tif response.status_code==200:\n",
    "\t\tbeautifulsoup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\t\tarticle_paragraphs = beautifulsoup.find_all(\"p\")\n",
    "\n",
    "\t\t# Cleaning article title\n",
    "\t\tarticle_title = re.sub(' ', '_', article['title'])\n",
    "\n",
    "\t\twith open(file=f\"../knowledge-bases/{keywords}_{source}/{article_title}.txt\", mode=\"w\") as fp:\n",
    "\t\t\tfor paragraph in article_paragraphs:\n",
    "\t\t\t\tparagraph_cleaned = str(paragraph.get_text()).strip()\n",
    "\n",
    "\t\t\t\tif paragraph_cleaned != \"\":\n",
    "\t\t\t\t\tfp.write(paragraph_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and Splitting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 485, which is longer than the specified 256\n",
      "Created a chunk of size 293, which is longer than the specified 256\n"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Defining directory path\n",
    "directory = f\"../knowledge-bases/{keywords}_{source}\"\n",
    "\n",
    "# Creating tokenizer\n",
    "\n",
    "# Creating loader and splitter\n",
    "loader = DirectoryLoader(path=directory, glob=\"*.txt\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(encoding_name=\"cl100k_base\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t   chunk_size=256, chunk_overlap=0)\n",
    "\n",
    "# Loading and splitting documents\n",
    "docs = loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding and Storing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Creating embeddings model\n",
    "embeddings = DartmouthEmbeddings(model_name=embeddings_model_name, dartmouth_api_key=str(DARTMOUTH_API_KEY))\n",
    "\n",
    "# Embedding documents and storing them in memory\n",
    "vector_store = InMemoryVectorStore(embedding=embeddings)\n",
    "\n",
    "for i in range(0, len(docs), 50):\n",
    "\t_ = vector_store.add_documents(docs[i: i+100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "{'detail': 'You\\'ve exceeded the daily usage limit (1000 credits) for the paid AI models.\\n                    IMPORTANT: Click the \"New Chat\" button and select one of the free models (ex. Llama 3.1) to start a new chat session.\\n                    '}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly respond with the classification.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Querying LLM and printing response to file\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(prompt)\n\u001b[1;32m     38\u001b[0m output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mpretty_repr() \u001b[38;5;241m+\u001b[39m output\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_dartmouth/llms.py:666\u001b[0m, in \u001b[0;36mChatDartmouthCloud.invoke\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Invokes the model to get a response to a query.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    See `LangChain's API documentation <https://python.langchain.com/v0.1/docs/expression_language/interface/>`_ for details on how to use this method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m    :rtype: BaseMessage\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:307\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    303\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    304\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    306\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    308\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    309\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    310\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    311\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    312\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    313\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    314\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    315\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    316\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    317\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:843\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    837\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    841\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    842\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_dartmouth/llms.py:688\u001b[0m, in \u001b[0;36mChatDartmouthCloud.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:683\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    682\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 683\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    684\u001b[0m                 m,\n\u001b[1;32m    685\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    686\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    687\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    688\u001b[0m             )\n\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:908\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 908\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    909\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    910\u001b[0m         )\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:824\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    823\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:858\u001b[0m, in \u001b[0;36mBaseChatOpenAI._create_chat_result\u001b[0;34m(self, response, generation_info)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# Sometimes the AI Model calling will get error, we should raise it.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# Otherwise, the next code 'choices.extend(response[\"choices\"])'\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# will throw a \"TypeError: 'NoneType' object is not iterable\" error\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# to mask the true error. Because 'response[\"choices\"]' is None.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(response_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    860\u001b[0m token_usage \u001b[38;5;241m=\u001b[39m response_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m response_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: {'detail': 'You\\'ve exceeded the daily usage limit (1000 credits) for the paid AI models.\\n                    IMPORTANT: Click the \"New Chat\" button and select one of the free models (ex. Llama 3.1) to start a new chat session.\\n                    '}"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "from langchain_dartmouth.llms import ChatDartmouthCloud\n",
    "import json\n",
    "\n",
    "# Initializing variable to hold output\n",
    "output = \"\"\n",
    "\n",
    "# Initializing variable referencing LLM\n",
    "llm = ChatDartmouthCloud(model_name=llm_model_name)\n",
    "\n",
    "# Open testing data file\n",
    "with open(testing_data, 'r') as fp:\n",
    "\ttest_data = json.load(fp)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "# Iterating through each test data point\n",
    "for tweet in test_data:\n",
    "\n",
    "\t# Retrieving most-similar documents\n",
    "\tquery = tweet['Tweet']\n",
    "\tdocs = vector_store.similarity_search(query, k=5)\n",
    "\t\n",
    "\t# Creating augmented prompt\n",
    "\tprompt = (\n",
    "\t\t\"Classify this tweet as 'Real News' or 'Fake News': \"\n",
    "\t\t+ query\n",
    "\t\t+ f\"\\n\\nConsider the following info: \\n\\n\"\n",
    "\t)\n",
    "\n",
    "\tfor doc in docs:\n",
    "\t\tprompt += doc.page_content + \"\\n--\\n\"\n",
    "\t\n",
    "\tprompt = prompt + \"Only respond with the classification.\"\n",
    "\n",
    "\t# Querying LLM and printing response to file\n",
    "\tresponse = llm.invoke(prompt)\n",
    "\toutput = response.pretty_repr() + output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, f1_score\n",
    "import io\n",
    "\n",
    "# Loading true labels\n",
    "with open(testing_data) as fp_input:\n",
    "\tinput_data = json.load(fp_input)\n",
    "\n",
    "\t# Encoding true labels as 0s and 1s\n",
    "\ty_actual = []\n",
    "\n",
    "\tfor entry in input_data:\n",
    "\t\tlabel = entry['Label']\n",
    "\n",
    "\t\tif label == \"Real News\":\n",
    "\t\t\ty_actual.append(1)\n",
    "\t\telif label == \"Fake News\":\n",
    "\t\t\ty_actual.append(0)\n",
    "\n",
    "# Extracting predicted labels\n",
    "y_pred = []\n",
    "\n",
    "# Encoding true labels as 0s and 1s\n",
    "with io.StringIO(output) as fp_output:\n",
    "\n",
    "\tfor line in fp_output:\n",
    "\t\tif \"Real News\" in line:\n",
    "\t\t\ty_pred.append(1)\n",
    "\t\telif \"Fake News\" in line:\n",
    "\t\t\ty_pred.append(0)\n",
    "\n",
    "# Calculating and printing metrics\n",
    "recall = recall_score(y_true=y_actual, y_pred=y_pred)\n",
    "precision = precision_score(y_true=y_actual, y_pred=y_pred)\n",
    "accuracy = accuracy_score(y_true=y_actual, y_pred=y_pred)\n",
    "f1score = f1_score(y_true=y_actual, y_pred=y_pred)\n",
    "confusion_matrix = confusion_matrix(y_true=y_actual, y_pred=y_pred)\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"F1 Score\", f1score)\n",
    "print(\"Confusion matrix: \",confusion_matrix)\n",
    "\t\t\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPT-4o-Mini w/o Augmentation** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "from langchain_dartmouth.llms import ChatDartmouthCloud\n",
    "import json\n",
    "\n",
    "# Initializing variable to hold output\n",
    "output = \"\"\n",
    "\n",
    "# Initializing variable referencing LLM\n",
    "llm = ChatDartmouthCloud(model_name=llm_model_name)\n",
    "\n",
    "# Open testing data file\n",
    "with open(testing_data, 'r') as fp:\n",
    "\ttest_data = json.load(fp)\n",
    "\n",
    "# Iterating through each test data point\n",
    "for tweet in test_data:\n",
    "\n",
    "\t# Creating non-augmented prompt\n",
    "\tquery = tweet['Tweet']\n",
    "\tprompt = (\n",
    "\t\t\"Classify this tweet as 'Real News' or 'Fake News': \"\n",
    "\t\t+ query\n",
    "\t\t+ \"Only respond with the classification.\"\n",
    "\t)\n",
    "\n",
    "\t# Querying LLM and printing response to file\n",
    "\tresponse = llm.invoke(prompt)\n",
    "\toutput = response.pretty_repr() + output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, f1_score\n",
    "import io\n",
    "\n",
    "# Loading true labels\n",
    "with open(testing_data) as fp_input:\n",
    "\tinput_data = json.load(fp_input)\n",
    "\n",
    "\t# Encoding true labels as 0s and 1s\n",
    "\ty_actual = []\n",
    "\n",
    "\tfor entry in input_data:\n",
    "\t\tlabel = entry['Label']\n",
    "\n",
    "\t\tif label == \"Real News\":\n",
    "\t\t\ty_actual.append(1)\n",
    "\t\telif label == \"Fake News\":\n",
    "\t\t\ty_actual.append(0)\n",
    "\n",
    "# Extracting predicted labels\n",
    "y_pred = []\n",
    "\n",
    "# Encoding true labels as 0s and 1s\n",
    "with io.StringIO(output) as fp_output:\n",
    "\n",
    "\tfor line in fp_output:\n",
    "\t\tif \"Real News\" in line:\n",
    "\t\t\ty_pred.append(1)\n",
    "\t\telif \"Fake News\" in line:\n",
    "\t\t\ty_pred.append(0)\n",
    "\n",
    "\n",
    "print(len(y_actual))\n",
    "print(len(y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculating and printing metrics\n",
    "recall = recall_score(y_true=y_actual, y_pred=y_pred)\n",
    "precision = precision_score(y_true=y_actual, y_pred=y_pred)\n",
    "accuracy = accuracy_score(y_true=y_actual, y_pred=y_pred)\n",
    "f1score = f1_score(y_true=y_actual, y_pred=y_pred)\n",
    "confusion_matrix = confusion_matrix(y_true=y_actual, y_pred=y_pred)\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"F1 Score\", f1score)\n",
    "print(\"Confusion matrix: \",confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
