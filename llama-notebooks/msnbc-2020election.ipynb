{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPT-4o-Mini w/ Augmentation** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "from config import DARTMOUTH_API_KEY, DARTMOUTH_CHAT_API_KEY\n",
    "from langchain_dartmouth.llms import DartmouthLLM\n",
    "\n",
    "# Retrieving keys and creating environment variables\n",
    "os.environ['DARTMOUTH_CHAT_API_KEY'] = DARTMOUTH_CHAT_API_KEY\n",
    "os.environ['DARTMOUTH_API_KEY'] = DARTMOUTH_API_KEY\n",
    "\n",
    "# Defining llm and embeddings models\n",
    "llm_model_name = \"llama-3-8b-instruct\"\n",
    "embeddings_model_name = \"bge-m3\"\n",
    "\n",
    "# Defining keywords and sources\n",
    "keywords = \"2020 election OR election fraud OR stop the steal OR voter fraud\"\n",
    "source=\"msnbc\"\n",
    "\n",
    "# Defining testing data file\n",
    "testing_data = '../input/2020election.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from newsapi import NewsApiClient\n",
    "import re\n",
    "\n",
    "# Creating directory to hold scraped articles\n",
    "os.makedirs(name=f\"../knowledge-bases/{keywords}_{source}\")\n",
    "\n",
    "# Creating newsapi client\n",
    "newsapiclient = NewsApiClient(api_key=\"03122dc3b7b84ea29212ca965b40c7aa\")\n",
    "\n",
    "# Querying articles\n",
    "articles = newsapiclient.get_everything(q=keywords, sources=source ,page_size=100)\n",
    "articles = articles['articles']\n",
    "\n",
    "# Scraping articles and saving in directory\n",
    "for article in articles:\n",
    "\turl = article['url']\n",
    "\tresponse = requests.get(url=url)\n",
    "\n",
    "\t# Printing article content to directory if valid response\n",
    "\tif response.status_code==200:\n",
    "\t\tbeautifulsoup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\t\tarticle_paragraphs = beautifulsoup.find_all(\"p\")\n",
    "\n",
    "\t\t# Cleaning article title\n",
    "\t\tarticle_title = re.sub(' ', '_', article['title'])\n",
    "\n",
    "\t\twith open(file=f\"../knowledge-bases/{keywords}_{source}/{article_title}.txt\", mode=\"w\") as fp:\n",
    "\t\t\tfor paragraph in article_paragraphs:\n",
    "\t\t\t\tparagraph_cleaned = str(paragraph.get_text()).strip()\n",
    "\n",
    "\t\t\t\tif paragraph_cleaned != \"\":\n",
    "\t\t\t\t\tfp.write(paragraph_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and Splitting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Defining directory path\n",
    "directory = f\"../knowledge-bases/{keywords}_{source}\"\n",
    "\n",
    "# Creating tokenizer\n",
    "\n",
    "# Creating loader and splitter\n",
    "loader = DirectoryLoader(path=directory, glob=\"*.txt\")\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(encoding_name=\"cl100k_base\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t   chunk_size=256, chunk_overlap=0)\n",
    "\n",
    "# Loading and splitting documents\n",
    "docs = loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding and Storing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "from langchain_dartmouth.embeddings import DartmouthEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Creating embeddings model\n",
    "embeddings = DartmouthEmbeddings(model_name=embeddings_model_name, dartmouth_api_key=str(DARTMOUTH_API_KEY))\n",
    "\n",
    "# Embedding documents and storing them in memory\n",
    "vector_store = InMemoryVectorStore(embedding=embeddings)\n",
    "\n",
    "for i in range(0, len(docs), 50):\n",
    "\t_ = vector_store.add_documents(docs[i: i+100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "import json\n",
    "\n",
    "# Initializing variable to hold output\n",
    "output = \"\"\n",
    "\n",
    "# Initializing variable referencing LLM\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")\n",
    "\n",
    "# Open testing data file\n",
    "with open(testing_data, 'r') as fp:\n",
    "\ttest_data = json.load(fp)\n",
    "\n",
    "# Iterating through each test data point\n",
    "for tweet in test_data:\n",
    "\n",
    "\t# Retrieving most-similar documents\n",
    "\tquery = tweet['Tweet']\n",
    "\tdocs = vector_store.similarity_search(query, k=5)\n",
    "\t\n",
    "\t# Creating augmented prompt\n",
    "\tprompt = (\n",
    "\t\t\"Classify tweet as 'Real News' or 'Fake News': \"\n",
    "\t\t+ query\n",
    "\t\t+ f\"\\n\\nConsider the following info: \\n\\n\"\n",
    "\t)\n",
    "\n",
    "\tfor doc in docs:\n",
    "\t\tprompt += doc.page_content + \"\\n--\\n\"\n",
    "\t\n",
    "\tprompt = prompt + \"Only respond with the classification.\"\n",
    "\n",
    "\t# Querying LLM and printing response to file\n",
    "\tresponse = llm.invoke(prompt)\n",
    "\toutput = response.pretty_repr() + output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.9090909090909091\n",
      "Precision:  0.47619047619047616\n",
      "Accuracy:  0.5\n",
      "F1 Score 0.625\n",
      "Confusion matrix:  [[ 4 22]\n",
      " [ 2 20]]\n"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, f1_score\n",
    "import io\n",
    "\n",
    "# Loading true labels\n",
    "with open(testing_data) as fp_input:\n",
    "\tinput_data = json.load(fp_input)\n",
    "\n",
    "\t# Encoding true labels as 0s and 1s\n",
    "\ty_actual = []\n",
    "\n",
    "\tfor entry in input_data:\n",
    "\t\tlabel = entry['Label']\n",
    "\n",
    "\t\tif label == \"Real News\":\n",
    "\t\t\ty_actual.append(1)\n",
    "\t\telif label == \"Fake News\":\n",
    "\t\t\ty_actual.append(0)\n",
    "\n",
    "# Extracting predicted labels\n",
    "y_pred = []\n",
    "\n",
    "# Encoding true labels as 0s and 1s\n",
    "with io.StringIO(output) as fp_output:\n",
    "\n",
    "\tfor line in fp_output:\n",
    "\t\tif \"Real News\" in line:\n",
    "\t\t\ty_pred.append(1)\n",
    "\t\telif \"Fake News\" in line:\n",
    "\t\t\ty_pred.append(0)\n",
    "\n",
    "# Calculating and printing metrics\n",
    "recall = recall_score(y_true=y_actual, y_pred=y_pred)\n",
    "precision = precision_score(y_true=y_actual, y_pred=y_pred)\n",
    "accuracy = accuracy_score(y_true=y_actual, y_pred=y_pred)\n",
    "f1score = f1_score(y_true=y_actual, y_pred=y_pred)\n",
    "confusion_matrix = confusion_matrix(y_true=y_actual, y_pred=y_pred)\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"F1 Score\", f1score)\n",
    "print(\"Confusion matrix: \",confusion_matrix)\n",
    "\t\t\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPT-4o-Mini w/o Augmentation** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import json\n",
    "\n",
    "# Initializing variable to hold output\n",
    "output = \"\"\n",
    "\n",
    "# Initializing variable referencing LLM\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")\n",
    "\n",
    "# Open testing data file\n",
    "with open(testing_data, 'r') as fp:\n",
    "\ttest_data = json.load(fp)\n",
    "\n",
    "# Iterating through each test data point\n",
    "for tweet in test_data:\n",
    "\n",
    "\t# Creating non-augmented prompt\n",
    "\tquery = tweet['Tweet']\n",
    "\tprompt = (\n",
    "\t\t\"Classify this tweet as 'Real News' or 'Fake News': \"\n",
    "\t\t+ query\n",
    "\t\t+ \"Only respond with the classification.\"\n",
    "\t)\n",
    "\n",
    "\t# Querying LLM and printing response to file\n",
    "\tresponse = llm.invoke(prompt)\n",
    "\toutput = response.pretty_repr() + output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.5909090909090909\n",
      "Precision:  0.48148148148148145\n",
      "Accuracy:  0.5208333333333334\n",
      "F1 Score 0.5306122448979592\n",
      "Confusion matrix:  [[12 14]\n",
      " [ 9 13]]\n"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "import json\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, f1_score\n",
    "import io\n",
    "\n",
    "# Loading true labels\n",
    "with open(testing_data) as fp_input:\n",
    "\tinput_data = json.load(fp_input)\n",
    "\n",
    "\t# Encoding true labels as 0s and 1s\n",
    "\ty_actual = []\n",
    "\n",
    "\tfor entry in input_data:\n",
    "\t\tlabel = entry['Label']\n",
    "\n",
    "\t\tif label == \"Real News\":\n",
    "\t\t\ty_actual.append(1)\n",
    "\t\telif label == \"Fake News\":\n",
    "\t\t\ty_actual.append(0)\n",
    "\n",
    "# Extracting predicted labels\n",
    "y_pred = []\n",
    "\n",
    "# Encoding true labels as 0s and 1s\n",
    "with io.StringIO(output) as fp_output:\n",
    "\n",
    "\tfor line in fp_output:\n",
    "\t\tif \"Real News\" in line:\n",
    "\t\t\ty_pred.append(1)\n",
    "\t\telif \"Fake News\" in line:\n",
    "\t\t\ty_pred.append(0)\n",
    "\n",
    "# Calculating and printing metrics\n",
    "recall = recall_score(y_true=y_actual, y_pred=y_pred)\n",
    "precision = precision_score(y_true=y_actual, y_pred=y_pred)\n",
    "accuracy = accuracy_score(y_true=y_actual, y_pred=y_pred)\n",
    "f1score = f1_score(y_true=y_actual, y_pred=y_pred)\n",
    "confusion_matrix = confusion_matrix(y_true=y_actual, y_pred=y_pred)\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"F1 Score\", f1score)\n",
    "print(\"Confusion matrix: \",confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
